a:30:{i:0;a:3:{i:0;s:14:"document_start";i:1;a:0:{}i:2;i:0;}i:1;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:0;}i:2;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:1;}i:3;a:3:{i:0;s:12:"section_edit";i:1;a:4:{i:0;i:-1;i:1;i:0;i:2;i:1;i:3;s:0:"";}i:2;i:1;}i:4;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:63:"Select features of a gene expression array via cross-validation";i:1;i:1;i:2;i:1;}i:2;i:1;}i:5;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:1;}i:2;i:1;}i:6;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:78;}i:7;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:233:"
For a classification problem, usually we wish to use as less variables as possible because of difficulties brought by the high dimension. Here we present the process of finding the minimum number of features for a predictive model. ";}i:2;i:79;}i:8;a:3:{i:0;s:6:"smiley";i:1;a:1:{i:0;s:5:"FIXME";}i:2;i:312;}i:9;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:317;}i:10;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:319;}i:11;a:3:{i:0;s:12:"section_edit";i:1;a:4:{i:0;i:1;i:1;i:318;i:2;i:1;i:3;s:63:"Select features of a gene expression array via cross-validation";}i:2;i:319;}i:12;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:9:"Animation";i:1;i:2;i:2;i:319;}i:2;i:319;}i:13;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:319;}i:14;a:3:{i:0;s:6:"p_open";i:1;a:0:{}i:2;i:340;}i:15;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:341;}i:16;a:3:{i:0;s:6:"plugin";i:1;a:3:{i:0;s:9:"animation";i:1;a:2:{i:0;i:1;i:1;a:7:{i:0;s:4:"gene";i:1;s:66:"http://i288.photobucket.com/albums/ll181/xieyihui/cv_nfeaturesLDA/";i:2;s:3:"png";i:3;s:2:"50";i:4;s:3:"480";i:5;s:3:"0.5";i:6;s:62:"Cross-validation to find the optimum number of features in LDA";}}i:2;i:1;}i:2;i:342;}i:17;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:497;}i:18;a:3:{i:0;s:6:"plugin";i:1;a:3:{i:0;s:9:"animation";i:1;a:2:{i:0;i:3;i:1;a:1:{i:0;s:176:"This animation has provided an illustration of the process of finding out the optimum number of variables using k-fold cross-validation in a linear discriminant analysis (LDA).";}}i:2;i:3;}i:2;i:498;}i:19;a:3:{i:0;s:5:"cdata";i:1;a:1:{i:0;s:1:"
";}i:2;i:674;}i:20;a:3:{i:0;s:6:"plugin";i:1;a:3:{i:0;s:9:"animation";i:1;a:2:{i:0;i:4;i:1;a:1:{i:0;s:0:"";}}i:2;i:4;}i:2;i:675;}i:21;a:3:{i:0;s:7:"p_close";i:1;a:0:{}i:2;i:681;}i:22;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:683;}i:23;a:3:{i:0;s:12:"section_edit";i:1;a:4:{i:0;i:319;i:1;i:682;i:2;i:2;i:3;s:9:"Animation";}i:2;i:683;}i:24;a:3:{i:0;s:6:"header";i:1;a:3:{i:0;s:6:"R code";i:1;i:2;i:2;i:683;}i:2;i:683;}i:25;a:3:{i:0;s:12:"section_open";i:1;a:1:{i:0;i:2;}i:2;i:683;}i:26;a:3:{i:0;s:4:"code";i:1;a:2:{i:0;s:529:"
oopt = ani.options(ani.height = 480, ani.width = 600, outdir = getwd(),
    interval = 0.5, nmax = 10,
    title = "Cross-validation to find the optimum number of features in LDA",
    description = "This animation has provided an illustration of the process of
    finding out the optimum number of variables using k-fold cross-validation
    in a linear discriminant analysis (LDA).")
ani.start()
par(mar = c(3, 3, 1, 0.5), mgp = c(1.5, 0.5, 0), tcl = -0.3, pch = 19, cex = 1.5)
cv.nfeaturesLDA()
ani.stop()
ani.options(oopt)
";i:1;N;}i:2;i:708;}i:27;a:3:{i:0;s:13:"section_close";i:1;a:0:{}i:2;i:1246;}i:28;a:3:{i:0;s:12:"section_edit";i:1;a:4:{i:0;i:683;i:1;i:0;i:2;i:2;i:3;s:6:"R code";}i:2;i:1246;}i:29;a:3:{i:0;s:12:"document_end";i:1;a:0:{}i:2;i:1246;}}