%% LyX 1.6.5 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{book}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[letterpaper]{geometry}
\geometry{verbose,lmargin=3cm,rmargin=3cm}
\usepackage{fancyhdr}
\pagestyle{fancy}
\setcounter{secnumdepth}{3}
\setcounter{tocdepth}{3}
\usepackage{color}
\usepackage{babel}

\usepackage{url}
\usepackage{amstext}
\usepackage{graphicx}
\usepackage[authoryear]{natbib}
\usepackage[unicode=true, 
 bookmarks=true,bookmarksnumbered=true,bookmarksopen=true,bookmarksopenlevel=3,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=true]
 {hyperref}
\hypersetup{pdftitle={ISU.Stat500: An R Package for the Course Stat500 },
 pdfauthor={Yihui Xie, Hui Lin, Tengfei Yin},
 pdfsubject={Statistics},
 pdfkeywords={R, package, statistical methods, Iowa State University},
 pdfstartview=FitH}
 
\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
%\VignetteIndexEntry{ISU.Stat500: An R Package for the Course Stat500 ``Statistical Methods''}
\usepackage{ae}
\usepackage{animate}
\renewcommand{\rmdefault}{ppl}
\renewcommand{\sfdefault}{aess}
\renewcommand{\ttdefault}{aett}
\newcommand{\pkg}[1]{\textbf{\textsf{#1}}}
\newcommand{\fun}[1]{\textit{#1}}

\makeatother

\begin{document}

\title{ISU.Stat500: An R Package for the Course Stat500 {}``Statistical
Methods''}


\author{Yihui Xie %
\thanks{Department of Statistics, Iowa State University. Email: \protect\href{mailto:xie@iastate.edu}{xie@iastate.edu}.
This document is based on the course STAT500 taught by \protect\href{http://www.public.iastate.edu/~pdixon}{Dr Dixon}
in Fall 2009. Comments on this document are warmly welcome.%
} \and Hui Lin %
\thanks{Department of Statistics, Iowa State University. Email: \protect\href{mailto:hlin@iastate.edu}{hlin@iastate.edu}.%
} \and Tengfei Yin %
\thanks{BCB (Bioinformatics and Computational Biology) Graduate Program, Iowa
State University. Email: \protect\href{mailto:tengfei@iastate.edu}{tengfei@iastate.edu}.%
}}

\maketitle
\tableofcontents{}

\listoffigures


<<echo=FALSE,results=hide>>=
options(prompt = "R> ",useFancyQuotes='TeX', SweaveHooks=list(fig=function() {
    par(las=1,mar=c(4,4,.1,.1),cex.lab=.95,cex.axis=.9,mgp=c(1.9,.7,0),tcl=-.3)
}))
source("commentSweave.r")
set.seed(0731) # the date I came to US :)
@

\SweaveOpts{prefix.string=fig, eps=FALSE}
\setkeys{Gin}{width=.85\textwidth}

\clearpage{}


\chapter{Introduction}

The R package \pkg{ISU.Stat500} is created to wrap up R code chunks
as functions which might be useful to the course {}``Statistical
Methods'' and illustrate the usage of R in several examples which
are included in the {}``\texttt{Examples}'' section of R help pages.
The content of this document covers t-test, ANOVA, linear regression
with their diagnostics, multifactor studies and categorical data methods.

To play with this package, please install it from CRAN if it is not
already in your library, and then load it into R:

<<>>=
## install if it does not exist: install.packages("ISU.Stat500")
library(ISU.Stat500)
@

This document is written in Sweave \citep{leisch02}, which makes
it reproducible. To reproduce this PDF document, just run Sweave on
the file \texttt{ISU.Stat500-intro.Rnw}, e.g. \texttt{R CMD Sweave
ISU.Stat500-intro.Rnw} or \texttt{Sweave(\textquotedbl{}ISU.Stat500-intro.Rnw\textquotedbl{})}
in R console and run \texttt{pdflatex ISU.Stat500-intro.tex}. As there
are a few very computer-intensive and time-consuming code chunks,
we've turned off their execution by setting the option \texttt{eval
= FALSE}, i.e. when you run Sweave, these code chunks will not be
evaluated; Figure \ref{fig:binom-norm-approx} is such an example.
As a convention of notation and typesetting, input R code is denoted
in \texttt{\emph{monospaced italics}} and begins with a prefix \texttt{\emph{R>}},
and output lines are using \texttt{normal monospaced font}.

We greatly appreciate Dr Dixon's excellent teaching in this semester
and we benefited a lot from his invaluable experience in statistical
practice.


\chapter{Comparative Studies}


\section{Two-sample t-test}


\subsection{Hypothesis test and confidence intervals}

<<>>=
x=read.table('platelet.txt',TRUE)
head(x)
fit=t.test(x$before,x$after,paired=TRUE)
fit
#get confidence interval
fit$conf.int
@


\subsection{Model diagnostics}


\subsubsection{Normality}
\begin{enumerate}
\item Shapiro-Wilk normality test


<<>>=
rainfall=read.table("rainfall.txt", header = TRUE)
head(rainfall)
tapply(rainfall$rain,rainfall$treatment,function(xx)shapiro.test(xx)$p.value) 
@

The p-values are close to 0, so the normal distribution is not reasonable.

\item Q-Q plot
\item Wilcoxon rank-sum test


<<>>=
wilcox.test(rain~treatment,rainfall,exact=FALSE)$p.value
@

\end{enumerate}

\subsubsection{Equal Variance}

Levene's test; \pkg{car} \citep{fox09}

<<>>=
library(car) 
(lvtest=levene.test(rain~treatment,data=rainfall))
@

The p-value is \Sexpr{round(lvtest$'Pr(>F)'[1],3)}, which shows an
evidence of heteroscedasticity (although not very strong).


\subsubsection{Remedies}
\begin{enumerate}
\item power transformation (HW3, 4(a))
\end{enumerate}

\section{Analysis of Variance}

ANOVA with blocks

<<>>=
platelet=read.table('platelet2.txt',TRUE)
platelet 
summary(aov(aggreg~time+factor(subject),platelet)) 
@

Contrast 
\begin{enumerate}
\item Data: Fat in Diets,


Interest in whether there is a diff{}erence between the extremely
low and fairly low diets; \pkg{gmodels} \citep{warnes09}

<<>>=
dietfat=read.table('dietfat.txt',TRUE,colClasses=c('factor','factor','numeric'))
dietfat
#install.packages("gmodel")
library(gmodels)
cmat = rbind(`EL vs FL` = c(1,-1,0))
fit=lm(lipid~fat+block, dietfat, contrasts = list(fat = make.contrasts(cmat)))
coef(summary(fit))
@

\end{enumerate}

\chapter{Linear Regression}


\section{Estimation and inference}

Model: $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\epsilon_{i}$; the goal is
to minimize $g(\beta_{0},\beta_{1})=\sum_{i=1}^{n}(Y_{i}-(\beta_{0}+\beta_{1}x_{i}))^{2}$.
Figure \ref{fig:OLS-slope} has illustrated the meaning of minimizing
SSE by changing the values of $\beta_{1}$ with $\beta_{0}$ fixed.

%
\begin{figure}
<<OLS-slope,eval=FALSE,fig=TRUE,width=6,height=4,include=FALSE>>=
library(animation)
par(mar = c(3.8, 3.5, 0.6, 0.1),mgp=c(1.9,.8,0)) 
ani.options(interval = 0, nmax = 50) 
## default animation: with slope changing 
least.squares() 
## we can also let the intercept change
## least.squares(ani.type = "i") 
@

\begin{centering}
\animategraphics[controls,width=.8\textwidth]{5}{fig-OLS-slope}{}{}
\par\end{centering}

\caption[Illustration of least squares by chaning $\beta_{1}$]{A demo of finding the slope by OLS with $\beta_{0}$ fixed. Cirles
denote data pairs $(x_{i},y_{i})$; the gray line is the real regression
line with $\beta_{1}=1$; the slope of the black line is changing
in order to find a $\hat{\beta}_{1}$ such that the sum of squared
errors (residuals, denoted by dashed vertical red lines) reaches its
minimum. SSE for the $i$-th step is drawn in the right plot (50 steps
in total). As the black line approaches to the gray line, the corresponding
SSE becomes smaller.\label{fig:OLS-slope}}

\end{figure}



\section{Model diagnostics}


\subsection{Linearity}

LOWESS/LOESS can capture local characteristics of data, and {}``linearity''
is a global characteristic. When linearity holds globally, it must
hold locally too. Therefore, we can compare the global fitting (linear
models%
\footnote{{}``linear'' here is in terms of X variables instead of parameters.%
}) to the local fitting (LOESS) to assess departure from linearity.
LOESS is almost equivalent to the linear modelwhen we set the smoothing
parameter to be infinitely large and fit local linear regressions
(polynomials with degree 1). A tutorial we find useful to further
understand LOESS is one of the lecture notes by William Jacoby%
\footnote{\url{http://polisci.msu.edu/jacoby/icpsr/regress3/lectures/week4/15.Loess.pdf}%
}, and there is also an intuitive demo in the \pkg{TeachingDemos}
package \citep{snow09} which shows the weights actually assigned
to each data point for local fitting (see \texttt{?loess.demo}).

The objects returned by the function \fun{loess()} can be compared
by \fun{anova()}. We take the \texttt{temperature} data as an example:

<<>>=
temp=read.table('temperature.txt',TRUE)
## span -> Inf, degree = 1: LOESS <==> linear regression model
loess.temp.lm=loess(temp~.,temp,span=10000,degree=1)
loess.temp=loess(temp~.,temp)
anova(loess.temp,loess.temp.lm)
## to show the effect of span = 10000, degree = 1, we fit a linear model
## and see the difference in fitted values (ideally should be 0)
loess.lm=lm(temp~.,temp) 
## within a tolerance of 1e-7, they are equal
all.equal(unname(fitted(loess.temp.lm)),unname(fitted(loess.lm)),tolerance=1e-7)
@

Added variable plots

%
\begin{figure}
<<addvar-plot,fig=TRUE,height=4,width=4,include=FALSE>>=
website<-read.table("website.txt",T)
lm.web<-lm(deliver~backlog+experience+process+year,website)
library(car)
av.plots(lm.web,"experience",identify.points=FALSE,main='')
@

\begin{centering}
\includegraphics[width=0.6\textwidth]{fig-addvar-plot}
\par\end{centering}

\caption{Added variable plot for the \texttt{website} data}



\end{figure}



\subsection{Independent / uncorrelated errors}

Durbin-Watson

Example: Test for a significant lag-1 correlation using the Durbin-Watson
test.

Use package: \pkg{lmtest} \citep{zeileis02}

<<>>=
library(lmtest)
lm.temp=lm(temp~year,temp)
dwtest(lm.temp)
@

Estimation in an AR(1) model

Use package: \pkg{nlme} \citep{pinheiro09} 

<<>>=
library(nlme)
gls.temp<-gls(temp~year,temp,correlation = corAR1())
summary(gls.temp)
@


\subsection{Constant variance}

Use residual plots.

%
\begin{figure}
<<temp-residual,fig=TRUE,height=4,width=4,include=FALSE>>=
plot(lm.temp,which=1)
@

\begin{centering}
\includegraphics[width=0.6\textwidth]{fig-temp-residual}
\par\end{centering}

\caption{Residual plot for the \texttt{temperature} data}



\end{figure}




Six plots (selectable by which) are currently available: a plot of
residuals against fitted values, a Scale-Location plot of \textbackslash{}sqrt\{|
residuals |\} against fitted values, a Normal Q-Q plot, a plot of
Cook's distances versus row labels, a plot of residuals against leverages,
and a plot of Cook's distances against leverage/(1-leverage). By default,
the first three and 5 are provided.

Breusch-Pagan

<<>>=
library(lmtest)
bptest(lm.web,~experience,data=website)
@


\section{Other types of linear models}


\subsection{Polynomial regression}
\begin{enumerate}
\item $Y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\beta_{3}x_{i}^{3}+...+\beta_{p}x_{i}^{p}+\epsilon_{i}$

\begin{enumerate}
\item $Y_{i}=\beta_{0}+\beta_{1}x_{i1}+\beta_{2}x_{i2}+\beta_{11}x_{i1}^{2}+\beta_{22}x_{i2}^{2}+\beta_{12}x_{i1}x_{i2}+\epsilon_{i}$


<<>>=
avp=read.table("avp.txt",T)
full.avp<-lm(y~x1+x2+x3+x1*x2+x1*x3+x2*x3+I(x1^2)+I(x2^2)+I(x3^2),data=avp)
summary(full.avp)
@

\end{enumerate}
\item Inference on $X_{max}$ for model: $Y_{i}=\beta_{0}+\beta_{1}X_{i}+\beta_{2}X_{i}^{2}+\epsilon_{i}$

\begin{itemize}
\item self-function
\end{itemize}
\end{enumerate}

\subsection{Categorical variables}
\begin{itemize}
\item use package: \pkg{nnet} \citep{venables02} 

\begin{itemize}
\item The function \textquotedbl{}class.ind\textquotedbl{} is used to generate
indicator variables
\item example
\end{itemize}
\end{itemize}

\subsection{Analysis of covariance}


\section{Model selection}

Stepwise -- By $C_{p}$, AIC, BIC, p-value

TODO-XIE: selection by PRESS (10th HW)


\chapter{Multifactor Studies}


\section{Factorial Designs}


\section{Random Effects}

TODO-XIE: extract variance components in an \texttt{mer} object


\chapter{Categorical Data Methods}


\section{Inference for a single proportion\label{sec:single-proportion}}

We want to make inference on the proportion parameter $\pi$ from
iid $Y_{i}\sim\text{Bernoulli}(\pi),\, i=1,2,\cdots,n$. The sample
mean (proportion) is $p=\sum_{i=1}^{n}Y_{i}/n$. We know $\sum Y_{i}\sim\text{Bin}(n,\pi)$,
so $E(p)=\pi$ and $Var(p)=\pi(1-\pi)/n$.


\subsection{Approximation from Binomial to Normal distribution}

The inference on $\pi$ can be based on the asymptotic Normal distribution
$N(\pi,\pi(1-\pi)/n)$. We may ask how good the approximation really
is, and Figure \ref{fig:binom-norm-approx} gives an illustration
on this issue. The quality of approximation is characterized by P-values
from KS test: larger P-values indicate better approximation. The rough
idea we get from the graph is we need larger $n$ (recall the Central
Limit Theorem), but we can also use relatively small $n$ with a $\pi$
near $0.5$ (say, $n=50$ and $\pi=0.6$).

%
\begin{figure}
<<binom-norm-approx,eval=FALSE,fig=TRUE,width=7,height=6,include=FALSE>>=
par(mar=c(4,4,.5,1.5))
## n from 1 to 100, pi from 0 to 1 (very intensive computation!)
pval=binom.norm.approx(1:100,seq(0,1,.005),xlab='n (number of trials)',ylab=expression(pi~'(probability of success)'))
@

\begin{centering}
\includegraphics[width=0.8\linewidth]{fig-binom-norm-approx}
\par\end{centering}

\caption[P-values from KS test to check normality of Bernoulli means]{P-values from Kolmogorov-Smirnov distribution test: for each combination
of $n$ and $\pi$, we can generate $n$ Bernoulli random numbers
(i.e. 0's and 1's) and compute the sample mean; repeat this procedure
for \texttt{nrep} times and we will get \texttt{nrep} sample mean
values. Then we can perform a Normal distribution test on these sample
means. Finally we obtain p-values for all combinations of $n$ and
$\pi$, which are visualized in a colored contour plot with colors
changing from cyan to pink, denoting p-values from small to large.
The breaks chosen for the contour plot are \texttt{c(0, 0.01, 0.05,
0.1, 0.2, 1)} so we roughly know the approximation to Normal distribution
is good in the light pink and pink area (where $n$ is relatively
large and $\pi$ is not too close to $0$ or $1$, but as $n$ grows
larger and larger, the restriction on $\pi$ becomes less important).
See the help page of \fun{binom.norm.approx()} in \pkg{ISU.Stat500}
for details.\label{fig:binom-norm-approx}}

\end{figure}


A rule of thumb to guarantee a good approximation as mentioned in
this course is $n\pi\geq5$ and $n(1-\pi)\geq5$ (equivalently $5\leq n\pi\leq n-5$),
and we can see what are the P-values corresponding to this rule in
Figure \ref{fig:binom-norm-condition}.

%
\begin{figure}
<<binom-norm-condition,eval=FALSE,fig=TRUE,width=7,height=6,include=FALSE>>=
## n * pi and n * (1 - pi)
np=c(outer(pval$n, pval$p, '*')) 
n1p=c(outer(pval$n, 1-pval$p, '*')) 
## condition for good approximation
cond=(np>=5) & (n1p>=5) 
## plot with semi-transparent colors: green if cond == TRUE, red else
plot(np,c(pval$p.value),pch=20,col=ifelse(cond,rgb(0,1,0,.2),rgb(1,0,0,.2)), xlab=expression('n *'~pi), ylab = 'P-value') 
@

\begin{centering}
\includegraphics[width=0.8\linewidth]{fig-binom-norm-condition}
\par\end{centering}

\caption[A rule of thumb to approximate the Normal distribution well]{A rule of thumb to approximate the Normal distribution well: we plot
the P-values against $np$ and mark out the points with $5\leq np\leq n-5$
in green; obviously this rule has removed the combinations of $n$
and $\pi$ which generates small P-values. Note there are some points
in the bottom-left corner whose P-values are large but still get removed
(a closer look at the these points reveals that either their $\pi$'s
are close to 0 or 1 and $n$'s are very large, or $\pi$'s close to
0.5 and $n$ is small). \label{fig:binom-norm-condition}}



\end{figure}


When the Normal distribution is a reasonable approximation, we can
compute confidence intervals for $\pi$ based on $p$ and perform
hypothesis tests like $\pi=\pi_{0}$ (bear in mind that $(p-E(p))/\sqrt{Var(p)}\sim N(0,1)$).


\subsection{Sample size computation}

Let the half-width of CI be $\delta=z_{1-\alpha/2}\sqrt{\pi(1-\pi)/n}\Rightarrow n=z_{1-\alpha/2}^{2}\pi(1-\pi)/\delta^{2}$.

<<>>=
alpha=.05;p=.1;delta=.1/2
(n=qnorm(1-alpha/2)^2*p*(1-p)/delta^2)
@

Let the power of test $\pi=\pi_{0}$ be $1-\beta$ and the true proportion
is $\pi_{a}$, then under confidence level $1-\alpha$:

\[
n=\frac{(z_{1-\alpha}\sqrt{\pi_{0}(1-\pi_{0})}+z_{1-\alpha}\sqrt{\pi_{a}(1-\pi_{a})})^{2}}{(\pi_{0}-\pi_{a})^{2}}\]


<<>>=
p0=.1;pa=.15;beta=.2;alpha=.05
(n=(qnorm(1-alpha)*sqrt(p0*(1-p0))+qnorm(1-beta)*sqrt(pa*(1-pa)))^2/(p0-pa)^2)
@


\section{Inference for two proportions}

Similar to Section \ref{sec:single-proportion}: $p_{1}-p_{2}\sim N(\pi_{1}-\pi_{2},\frac{\pi_{1}(1-\pi_{1})}{n_{1}}+\frac{\pi_{2}(1-\pi_{2})}{n_{2}})$.


\section{Odds and odds ratio}

Odds is the ratio of success rate to failure rate: $\pi/(1-\pi)$;
odds ratio is the ratio of two odds $\phi=(\pi_{1}/(1-\pi_{1}))/(\pi_{2}/(1-\pi_{2}))$;
$\log\phi$ is commonly used.

\[
\log\hat{\phi}\sim N(\log\phi,\frac{1}{n_{1}\pi_{1}(1-\pi_{1})}+\frac{1}{n_{2}\pi_{2}(1-\pi_{2})})\]


\[
\hat{Var}\log\hat{\phi}=\frac{1}{Y_{1}}+\frac{1}{n_{1}-Y_{1}}+\frac{1}{Y_{2}}+\frac{1}{n_{2}-Y_{2}}\]


Odds lies in $(0,+\infty)$ while $\pi\in[0,1]$, which makes odds
{}``safer'' in prediction than the original proportion (predicted
$\pi$ might exceed the range $[0,1]$).


\section{Contingency tables}


\chapter{Misc Functions}

We have also collected some functions in \pkg{ISU.Stat500} which
are of no direct interest to statistical methods, but they might be
helpful for us to save time, and we will also give some tips on the
usage of existing functions in base R.


\section{Making better use of \texttt{read.table()}}

The function \fun{read.table()} (or \fun{read.csv()} and other \fun{read.*()}
functions) is usually used to read pure text data into R. Here are
some tips on making more advantage of them:
\begin{enumerate}
\item specify column classes \texttt{colClasses} if you already know them:
it will save your time explicitly converting some columns to factors%
\footnote{R will convert characters to factors by default; to stop this conversion,
use \texttt{read.table(..., stringsAsFactors = FALSE)}%
} or characters, etc and keep the code neat. For example, we know the
first 4 columns of the data \texttt{freeway.txt} are categorical variables
and the last column is numeric:


<<>>=
str(freeway1 <- read.table("freeway.txt", TRUE, colClasses = c(rep("factor", 4), "double")))
## or explicitly convert:
freeway = read.table("freeway.txt", TRUE)
freeway$rod = factor(freeway$rod)
freeway$segment = factor(freeway$segment)
freeway$lane = factor(freeway$lane)
freeway$location = factor(freeway$location)
## do the two approaches get exactly the same data?
identical(freeway1, freeway)
@

\item we can skip a certain number of rows and read a limited number of
rows of a data file: this can be useful when both data and its explanations
are mixed in one file. For example, we know the real data in \texttt{lab1bigex1.txt}
starts from the 4th row and lasts for 50 rows, then begins again from
57th row till the end of the file, so we can read the numbers like
this:


<<>>=
# check the first 5 lines
readLines('lab1bigex1.txt',5)
## we only need a vector of values, so the low-level scan() suffices
str(scan('lab1bigex1.txt', skip=3,nlines=50))
str(scan('lab1bigex1.txt', skip=57))
## 'nrows' defines the number of rows in read.table():
## read.table('lab1bigex1.txt', skip = 3, nrows = 50)
@

\item the argument \texttt{fill} can help us fill blank cells (useful when
some data lines are not complete);
\item we can specify a certain column to be the row names by \texttt{row.names},
e.g. \texttt{row.names = 1} (the 1st column); and we can also set
\texttt{col.names} to be column names of the data (useful when there
is no header line but we know the variable names already; otherwise
use \texttt{colnames(dat) = ...} after \texttt{dat = read.table()});
\item the first argument \texttt{file} can be very flexible, e.g. a file
in the local drive, a URL, a compressed gzip file (which can save
space and reading will not be much slower than uncompressed files);


<<eval=FALSE>>=
read.table('http://pdixon.public.iastate.edu/stat500/data/creat.txt')
@

\end{enumerate}

\section{Dynamic access to course materials}

The function \fun{listLinks()} can list all the hyper-links under
a web directory with the help of the \pkg{XML} package \citep{templelang09}.
For example, we obtain the names of SAS programs and output files:

<<eval=FALSE>>=
listLinks("http://pdixon.public.iastate.edu/stat500/sas/", "\\.lst$|\\.sas$") 
## will get: 
#  [1] "http://pdixon.public.iastate.edu/stat500/sas/ancova.sas"
#  [2] "http://pdixon.public.iastate.edu/stat500/sas/bacillus.lst"
#  [3] "http://pdixon.public.iastate.edu/stat500/sas/bacillus.sas"
#  ....
## data files are under "http://www.public.iastate.edu/~pdixon/stat500/data/"
@

We can further download these files automatically using R. There are
several examples in the help page; see \texttt{?listLinks}.


\section[Reshaping data]{Reshaping data between wide and long formats}

The function \fun{reshape()} in the \pkg{stats} package can reshape
data between {}``wide'' and {}``long'' formats. For the rat weight
data, one single line of R code is enough for the transformation%
\footnote{Explanations can be found at \url{http://cos.name/en/topic/reshape-the-ratweight-data-into-long-format-stat500}
if needed.%
}.

<<>>=
rat = read.table("ratweight.txt", col.names = c("amount", "type", rep("gain", 10))) 
rat2 = reshape(rat, varying = list(3:12), idvar = 1:2, direction = "long") 
head(rat,5)
head(rat2,5)
@

\bibliographystyle{jss}
\cleardoublepage\addcontentsline{toc}{chapter}{\bibname}\bibliography{ISU-Stat500-intro}

\end{document}
